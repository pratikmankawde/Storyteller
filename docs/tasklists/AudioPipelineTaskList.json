{
  "project": "Dramebaz",
  "category": "Audio Pipeline Enhancements",
  "description": "Enhanced audio pipeline features from NovelReaderWeb - Director Pipeline and TTS improvements",
  "version": "1.0",
  "last_updated": "2026-02-03",
  "instructions_source": "NovelReaderWeb/docs/ARCHITECTURE.md, NovelReaderWeb/docs/AI.md, NovelReaderWeb/docs/FEATURES.md",
  "reference_project": "C:\\Users\\Pratik\\source\\NovelReaderWeb",
  "total_estimated_hours": 12,
  "milestones": [
    {
      "id": "AUDIO-M1",
      "name": "Director Pipeline Enhancement",
      "description": "Implement Producer-Consumer pattern with Kotlin Channels for audio pipeline",
      "done": false,
      "task_ids": ["AUDIO-001"]
    },
    {
      "id": "AUDIO-M2",
      "name": "TTS Emotion Modifiers",
      "description": "Complete emotion-based TTS modulation with speed/pitch/volume adjustments",
      "done": false,
      "task_ids": ["AUDIO-002"]
    }
  ],
  "tasks": [
    {
      "id": "AUDIO-001",
      "feature": "Enhanced Director Pipeline",
      "description": "Implement Producer-Consumer audio pipeline with Kotlin Channels for better performance and latency.",
      "done": false,
      "status": "not_started",
      "priority": "HIGH",
      "estimated_hours": 8,
      "reference_docs": [
        "NovelReaderWeb/docs/ARCHITECTURE.md - Section 'The Director Pipeline'",
        "NovelReaderWeb/docs/FEATURES.md - Section 'AudioDirector'"
      ],
      "files_to_modify": [
        "app/src/main/java/com/dramebaz/app/playback/engine/PlaybackEngine.kt",
        "app/src/main/java/com/dramebaz/app/playback/engine/AudioDirector.kt (new)"
      ],
      "implementation_notes": "Create segmentChannel(capacity=10) and audioBufferChannel(capacity=5). Implement 3 coroutine jobs: Producer(LLM script generation), Transformer(TTS synthesis), Consumer(AudioTrack playback). Process next sentence while playing current one.",
      "work_items": [
        "Create AudioDirector class with Channel-based pipeline",
        "Implement Producer job: LLM script generation",
        "Implement Transformer job: TTS synthesis with speaker mapping",
        "Implement Consumer job: AudioTrack playback with UI sync",
        "Add buffered Channels for script segments and audio buffers",
        "Integrate with existing PlaybackEngine"
      ],
      "sub_tasks": [
        {"id": "AUDIO-001.1", "description": "Create AudioDirector.kt with Channel definitions", "done": false},
        {"id": "AUDIO-001.2", "description": "Implement Producer coroutine for LLM script generation", "done": false},
        {"id": "AUDIO-001.3", "description": "Implement Transformer coroutine for TTS synthesis", "done": false},
        {"id": "AUDIO-001.4", "description": "Implement Consumer coroutine for AudioTrack playback", "done": false},
        {"id": "AUDIO-001.5", "description": "Add UI sync emission for karaoke highlighting", "done": false},
        {"id": "AUDIO-001.6", "description": "Integrate AudioDirector into PlaybackEngine", "done": false}
      ],
      "code_pattern": "// From NovelReaderWeb docs/ARCHITECTURE.md\nclass AudioDirector(\n    private val llmService: LocalLlmService,\n    private val ttsService: SherpaTtsService\n) {\n    private val segmentChannel = Channel<SpeechSegment>(capacity = 10)\n    private val audioBufferChannel = Channel<AudioBuffer>(capacity = 5)\n\n    suspend fun startReading(pageText: String) = coroutineScope {\n        // Producer: LLM Script Generation\n        launch(Dispatchers.Default) {\n            val scriptJson = llmService.generateScript(pageText)\n            val segments = parseSegments(scriptJson)\n            for (seg in segments) { segmentChannel.send(seg) }\n            segmentChannel.close()\n        }\n\n        // Transformer: TTS Synthesis\n        launch(Dispatchers.Default) {\n            for (seg in segmentChannel) {\n                val speakerId = characterMap[seg.speaker] ?: 0\n                val audio = ttsService.generateAudio(seg.text, speakerId, seg.emotion)\n                audioBufferChannel.send(AudioBuffer(audio, seg))\n            }\n            audioBufferChannel.close()\n        }\n\n        // Consumer: AudioTrack Playback\n        launch(Dispatchers.IO) {\n            for (buffer in audioBufferChannel) {\n                audioTrack.write(buffer.pcm, 0, buffer.pcm.size)\n                emitActiveSegment(buffer.segment)\n            }\n        }\n    }\n}"
    },
    {
      "id": "AUDIO-002",
      "feature": "Complete TTS Emotion Modifiers",
      "description": "Implement full emotion-based TTS modulation with speed, pitch, and volume adjustments.",
      "done": false,
      "status": "not_started",
      "priority": "HIGH",
      "estimated_hours": 4,
      "reference_docs": [
        "NovelReaderWeb/docs/AI.md - Section 'TTS Emotion Modifiers'",
        "NovelReaderWeb/docs/AI.md - Section 'TTS Input Formatting'"
      ],
      "files_to_modify": [
        "app/src/main/java/com/dramebaz/app/ai/tts/SherpaTtsEngine.kt",
        "app/src/main/java/com/dramebaz/app/data/models/EmotionModifier.kt (new)",
        "app/src/main/java/com/dramebaz/app/data/models/VoiceProfile.kt"
      ],
      "implementation_notes": "Create EmotionModifier data class with speed/pitch/volume multipliers. Map emotion tags to modifiers. Apply modifiers in SherpaTtsEngine.synthesize(). Add volume post-processing for whisper/angry effects.",
      "work_items": [
        "Create EmotionModifier data class",
        "Define EMOTION_MODIFIERS map with all emotion types",
        "Update SherpaTtsEngine.synthesize() to accept emotion parameter",
        "Apply speed modifier to TTS generation",
        "Apply volume modifier as post-processing on PCM data",
        "Test all emotion types"
      ],
      "sub_tasks": [
        {"id": "AUDIO-002.1", "description": "Create EmotionModifier.kt data class", "done": false},
        {"id": "AUDIO-002.2", "description": "Define EMOTION_MODIFIERS constant map", "done": false},
        {"id": "AUDIO-002.3", "description": "Update synthesize() to apply speed modifier", "done": false},
        {"id": "AUDIO-002.4", "description": "Add volume post-processing for PCM data", "done": false},
        {"id": "AUDIO-002.5", "description": "Test emotion modifiers with sample text", "done": false}
      ],
      "emotion_table": {
        "neutral": {"speed": 1.0, "pitch": 1.0, "volume": 1.0},
        "sad": {"speed": 0.8, "pitch": 0.9, "volume": 1.0},
        "angry": {"speed": 1.2, "pitch": 1.2, "volume": 1.2},
        "fear": {"speed": 1.1, "pitch": 1.3, "volume": 1.0},
        "whisper": {"speed": 0.9, "pitch": 1.0, "volume": 0.5},
        "happy": {"speed": 1.1, "pitch": 1.1, "volume": 1.0}
      },
      "code_pattern": "// From NovelReaderWeb docs/AI.md\ndata class EmotionModifier(\n    val speedMultiplier: Float,\n    val pitchMultiplier: Float,\n    val volumeMultiplier: Float\n)\n\nval EMOTION_MODIFIERS = mapOf(\n    \"neutral\" to EmotionModifier(1.0f, 1.0f, 1.0f),\n    \"sad\" to EmotionModifier(0.8f, 0.9f, 1.0f),\n    \"angry\" to EmotionModifier(1.2f, 1.2f, 1.2f),\n    \"fear\" to EmotionModifier(1.1f, 1.3f, 1.0f),\n    \"whisper\" to EmotionModifier(0.9f, 1.0f, 0.5f),\n    \"happy\" to EmotionModifier(1.1f, 1.1f, 1.0f)\n)"
    }
  ],
  "notes": {
    "pipeline_design": "Producer-Consumer pattern ensures audio generation happens ahead of playback without blocking UI",
    "latency_target": "First token < 1s, TTS RTF < 1.0 (faster than real-time)",
    "channel_capacity": "Script channel: 10 segments, Audio buffer channel: 5 buffers"
  }
}

