# Benchmark all GGUF models in the specified directory
# Uses llama-bench.exe from llama.cpp for performance benchmarks

param(
    [string]$ModelsDir = "D:\Learning\Ai\Models\LLM",
    [string]$OutputFile = "C:\Users\Pratik\source\Storyteller\scripts\benchmark_report.md"
)

$LlamaDir = "C:\Users\Pratik\source\Storyteller\scripts\llama-cpp"
$LlamaBench = Join-Path $LlamaDir "llama-bench.exe"
$LlamaCli = Join-Path $LlamaDir "llama-cli.exe"

# Verify llama-bench exists
if (-not (Test-Path $LlamaBench)) {
    Write-Error "llama-bench.exe not found at $LlamaBench"
    exit 1
}

# Find all GGUF models
Write-Host "Searching for GGUF models in $ModelsDir..."
$models = Get-ChildItem -Path $ModelsDir -Recurse -Filter "*.gguf" -ErrorAction SilentlyContinue
Write-Host "Found $($models.Count) models"

if ($models.Count -eq 0) {
    Write-Error "No GGUF models found in $ModelsDir"
    exit 1
}

# Start building report
$report = @"
# LLM Benchmark Report

**Generated:** $(Get-Date -Format "yyyy-MM-dd HH:mm:ss")
**System:** $env:COMPUTERNAME
**Models Directory:** $ModelsDir

## Summary

| Model | Size (GB) | PP (t/s) | TG (t/s) | PP Time | TG Time |
|-------|-----------|----------|----------|---------|---------|
"@

$detailedResults = @()

foreach ($model in $models) {
    $modelPath = $model.FullName
    $modelName = $model.Name
    $modelSizeGB = [math]::Round($model.Length / 1GB, 2)

    Write-Host ""
    Write-Host "=" * 60
    Write-Host "Benchmarking: $modelName ($modelSizeGB GB)"
    Write-Host "=" * 60

    # Run llama-bench with pp512 and tg128 tests
    try {
        $benchOutput = & $LlamaBench -m $modelPath -p 512 -n 128 -r 3 2>&1
        $benchText = $benchOutput -join "`n"

        # Parse results - format: | model | size | params | backend | threads | test | t/s |
        # Example line: | qwen3 1.7B Q8_0 | 1.70 GiB | 1.72 B | CPU | 6 | pp512 | 54.60 Â± 0.00 |
        $ppMatch = [regex]::Match($benchText, "pp512\s*\|\s*([\d.]+)\s*")
        $tgMatch = [regex]::Match($benchText, "tg128\s*\|\s*([\d.]+)\s*")

        $ppSpeed = if ($ppMatch.Success) { $ppMatch.Groups[1].Value } else { "N/A" }
        $tgSpeed = if ($tgMatch.Success) { $tgMatch.Groups[1].Value } else { "N/A" }

        # Calculate times
        $ppTime = if ($ppSpeed -ne "N/A" -and [double]$ppSpeed -gt 0) {
            [math]::Round(512 / [double]$ppSpeed, 2)
        } else { "N/A" }
        $tgTime = if ($tgSpeed -ne "N/A" -and [double]$tgSpeed -gt 0) {
            [math]::Round(128 / [double]$tgSpeed, 2)
        } else { "N/A" }

        $report += "`n| $modelName | $modelSizeGB | $ppSpeed | $tgSpeed | ${ppTime}s | ${tgTime}s |"

        $detailedResults += @{
            Name = $modelName
            Path = $modelPath
            SizeGB = $modelSizeGB
            PPSpeed = $ppSpeed
            TGSpeed = $tgSpeed
            RawOutput = $benchText
        }

        Write-Host "PP512: $ppSpeed t/s, TG128: $tgSpeed t/s"
    }
    catch {
        Write-Host "Error benchmarking $modelName : $_"
        $report += "`n| $modelName | $modelSizeGB | ERROR | ERROR | - | - |"
    }
}

# Add detailed results section
$report += @"


## Detailed Results

"@

foreach ($result in $detailedResults) {
    $report += @"

### $($result.Name)

- **Path:** $($result.Path)
- **Size:** $($result.SizeGB) GB
- **Prompt Processing (pp512):** $($result.PPSpeed) tokens/sec
- **Token Generation (tg128):** $($result.TGSpeed) tokens/sec

<details>
<summary>Raw Benchmark Output</summary>

``````
$($result.RawOutput)
``````

</details>

"@
}

# Add recommendations
$report += @"

## Recommendations

Based on the benchmark results:
- **Fastest for generation:** Model with highest TG (tokens/sec)
- **Fastest for prompt processing:** Model with highest PP (tokens/sec)
- **Best for mobile/embedded:** Smaller models with acceptable speed

---
*Report generated by benchmark_models.ps1*
"@

# Save report
$report | Out-File -FilePath $OutputFile -Encoding UTF8
Write-Host ""
Write-Host "=" * 60
Write-Host "Benchmark complete! Report saved to: $OutputFile"
Write-Host "=" * 60
