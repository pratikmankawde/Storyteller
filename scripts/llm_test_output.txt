=== LLM Pass-1 Character Extraction Test Results ===
Date: 2026-01-31
System: ROG-RIG (Windows PC, CPU: Haswell backend)

=== Test Configuration ===
PDF: Space story.pdf
Prompt: Pass-1 character extraction (same format as Android app)
Prompt length: 2368 characters
Text length: ~2000 characters (first portion of PDF)
Max tokens: 256
Temperature: 0.1
Context size: 4096

=== Model 1: qwen3-1.7b-q4_k_m.gguf (1.19 GB) ===
Status: SUCCESS
Prompt processing: 108.3 t/s
Token generation: 19.9 t/s
Output: {"characters": ["Jax", "Lyra", "Kael", "Zane"]}

=== Model 2: Qwen3-1.7B-Q8_0.gguf (1.71 GB) ===
Status: SUCCESS
Prompt processing: 56.6 t/s
Token generation: 14.2 t/s
Output: {"characters": ["Jax", "Lyra", "Kael", "Zane"]}

=== Comparison with Android App ===
The same Qwen3-1.7B-Q8_0.gguf model HANGS on Android but works fine on PC.

Possible causes for Android hanging:
1. Vulkan GPU backend issues on the mobile device
2. Memory constraints on mobile (model may not fit properly)
3. Different llama.cpp version/build between PC and Android JNI
4. Context size or batch size configuration differences
5. Native code threading issues in the Android JNI implementation

=== Recommendations ===
1. Try using the Q4_K_M quantization on Android (smaller, faster)
2. Check if the Android device has enough RAM for the Q8_0 model
3. Add timeout mechanism to the native JNI code
4. Consider reducing context size on Android
5. Check Vulkan driver compatibility on the Android device

=== Prompt Used ===
<|im_start|>system
You are a character name extraction engine. Extract ONLY character names. /no_think<|im_end|>
<|im_start|>user
STRICT RULES:
- Extract ONLY proper names explicitly written in the text
- Do NOT include pronouns or generic descriptions
- Output valid JSON only

Format: {"characters": ["Name1", "Name2"]}

TEXT:
[2000 chars of Space story.pdf text]
<|im_end|>
<|im_start|>assistant

=== Expected Output ===
{"characters": ["Jax", "Lyra", "Kael", "Zane"]}

